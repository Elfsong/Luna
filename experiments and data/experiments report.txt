We trained different LLMs and tested them on our Gold dataset with the objective of determining if the text corresponding to a metadata/case-note is susceptible of containing the software name. The following models were tried: Bert-large, Roberta-large, T5-large and Flan-T5-large. we display the classification report for each model.

Train data:train2_data.csv
Test data:test_gold_data.csv

Bert-large Results:

test Accuracy: 0.9567
                   precision    recall  f1-score   support

       False            0.98      0.97      0.98       234
        True            0.71      0.75      0.73        20

    accuracy                                0.96       254
   macro avg            0.85      0.86      0.85       254
weighted avg            0.96      0.96      0.96       254

Roberta-large results:

test Accuracy: 0.9567
                    precision    recall  f1-score   support

       False            0.99      0.97      0.98       234
        True            0.68      0.85      0.76        20

    accuracy                                0.96       254
   macro avg            0.83      0.91      0.87       254
weighted avg            0.96      0.96      0.96       254

T5-large balanced data:

                     precision    recall  f1-score   support

       False            0.98      0.83      0.90       234
        True            0.30      0.85      0.45        20

    accuracy                                0.83       254
   macro avg            0.64      0.84      0.68       254
weighted avg            0.93      0.83      0.87       254

T5-large unbalanced data:

                     precision    recall  f1-score   support

       False            0.99      0.94      0.96       234
        True            0.56      0.90      0.69        20

    accuracy                                0.94       254
   macro avg            0.78      0.92      0.83       254
weighted avg            0.96      0.94      0.94       254

flan-t5-large:

                       precision    recall  f1-score   support

       False              1.00      0.98      0.99       234
        True              0.83      1.00      0.91        20

    accuracy                                  0.98       254
   macro avg              0.92      0.99      0.95       254
weighted avg              0.99      0.98      0.98       254
